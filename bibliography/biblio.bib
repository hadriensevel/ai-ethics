@book{Coeckelbergh2020,
  author    = {Coeckelbergh, Mark},
  title     = {{AI Ethics}},
  publisher = {The MIT Press},
  year      = {2020},
  month     = {04},
  abstract  = {{An accessible synthesis of ethical issues raised by artificial intelligence that moves beyond hype and nightmare scenarios to address concrete questions.Artificial intelligence powers Google's search engine, enables Facebook to target advertising, and allows Alexa and Siri to do their jobs. AI is also behind self-driving cars, predictive policing, and autonomous weapons that can kill without human intervention. These and other AI applications raise complex ethical issues that are the subject of ongoing debate. This volume in the MIT Press Essential Knowledge series offers an accessible synthesis of these issues. Written by a philosopher of technology, AI Ethics goes beyond the usual hype and nightmare scenarios to address concrete questions.Mark Coeckelbergh describes influential AI narratives, ranging from Frankenstein's monster to transhumanism and the technological singularity. He surveys relevant philosophical discussions: questions about the fundamental differences between humans and machines and debates over the moral status of AI. He explains the technology of AI, describing different approaches and focusing on machine learning and data science. He offers an overview of important ethical issues, including privacy concerns, responsibility and the delegation of decision making, transparency, and bias as it arises at all stages of data science processes. He also considers the future of work in an AI economy. Finally, he analyzes a range of policy proposals and discusses challenges for policymakers. He argues for ethical practices that embed values in design, translate democratic values into practices and include a vision of the good life and the good society.}},
  isbn      = {9780262357067},
  doi       = {10.7551/mitpress/12549.001.0001},
  url       = {https://doi.org/10.7551/mitpress/12549.001.0001}
}

@inbook{Bartneck2021,
  author    = {Bartneck, Christoph
               and L{\"u}tge, Christoph
               and Wagner, Alan
               and Welsh, Sean},
  title     = {What Is Ethics?},
  chapter   = {3},
  booktitle = {An Introduction to Ethics in Robotics and AI},
  year      = {2021},
  publisher = {Springer International Publishing},
  pages     = {17--26},
  abstract  = {This chapter introduces the theories that form the basis of the ethical review of robots and AI systems. We introduce the major approaches to moral theory (deontology, consequentialism and virtue ethics) and discuss the relation of ethics to law. Finally, we discuss how these theories might be implemented in a machine to enable it to make ethical decisions.},
  isbn      = {978-3-030-51110-4},
  doi       = {10.1007/978-3-030-51110-4_3},
  url       = {https://doi.org/10.1007/978-3-030-51110-4_3}
}

@incollection{Müller2021,
  author       = {Müller, Vincent C.},
  title        = {{Ethics of Artificial Intelligence and Robotics}},
  booktitle    = {The {Stanford} Encyclopedia of Philosophy},
  editor       = {Edward N. Zalta},
  howpublished = {\url{https://plato.stanford.edu/archives/sum2021/entries/ethics-ai/}},
  year         = {2021},
  edition      = {{S}ummer 2021},
  publisher    = {Metaphysics Research Lab, Stanford University}
}

@incollection{Gebru2020,
  author    = {Gebru, Timnit},
  isbn      = {9780190067397},
  title     = {{Race and Gender}},
  booktitle = {{The Oxford Handbook of Ethics of AI}},
  publisher = {Oxford University Press},
  year      = {2020},
  month     = {07},
  abstract  = {{This chapter discusses the role of race and gender in artificial intelligence (AI). The rapid permeation of AI into society has not been accompanied by a thorough investigation of the sociopolitical issues that cause certain groups of people to be harmed rather than advantaged by it. For instance, recent studies have shown that commercial automated facial analysis systems have much higher error rates for dark-skinned women, while having minimal errors on light-skinned men. Moreover, a 2016 ProPublica investigation uncovered that machine learning–based tools that assess crime recidivism rates in the United States are biased against African Americans. Other studies show that natural language–processing tools trained on news articles exhibit societal biases. While many technical solutions have been proposed to alleviate bias in machine learning systems, a holistic and multifaceted approach must be taken. This includes standardization bodies determining what types of systems can be used in which scenarios, making sure that automated decision tools are created by people from diverse backgrounds, and understanding the historical and political factors that disadvantage certain groups who are subjected to these tools.}},
  doi       = {10.1093/oxfordhb/9780190067397.013.16},
  url       = {https://doi.org/10.1093/oxfordhb/9780190067397.013.16},
  eprint    = {https://academic.oup.com/book/0/chapter/290662826/chapter-ag-pdf/44521942/book\_34287\_section\_290662826.ag.pdf}
}

@article{Rochel2021,
  author   = {Rochel, Johan
              and Ev{\'e}quoz, Florian},
  title    = {Getting into the engine room: a blueprint to investigate the shadowy steps of AI ethics},
  journal  = {AI {\&} SOCIETY},
  year     = {2021},
  month    = {Jun},
  day      = {01},
  volume   = {36},
  number   = {2},
  pages    = {609-622},
  abstract = {Enacting an AI system typically requires three iterative phases where AI engineers are in command: selection and preparation of the data, selection and configuration of algorithmic tools, and fine-tuning of the different parameters on the basis of intermediate results. Our main hypothesis is that these phases involve practices with ethical questions. This paper maps these ethical questions and proposes a way to address them in light of a neo-republican understanding of freedom, defined as absence of domination. We thereby identify different types of responsibility held by AI engineers and link them to concrete suggestions on how to improve professional practices. This paper contributes to the literature on AI and ethics by focusing on the work necessary to configure AI systems, thereby offering an input to better practices and an input for societal debates.},
  issn     = {1435-5655},
  doi      = {10.1007/s00146-020-01069-w},
  url      = {https://doi.org/10.1007/s00146-020-01069-w}
}

@article{Poel2021,
  author   = {{van de Poel}, Ibo
              and Sand, Martin},
  title    = {Varieties of responsibility: two problems of responsible innovation},
  journal  = {Synthese},
  year     = {2021},
  month    = {Aug},
  day      = {01},
  volume   = {198},
  number   = {19},
  pages    = {4769-4787},
  abstract = {The notion of responsible innovation suggests that innovators carry additional responsibilities (to society, stakeholders, users) beyond those commonly suggested. In this paper, we will discuss the meaning of these novel responsibilities focusing on two philosophical problems of attributing such responsibilities to innovators. The first is the allocation of responsibilities to innovators. Innovation is a process that involves a multiplicity of agents and unpredictable, far-reaching causal chains from innovation to social impacts, which creates great uncertainty. A second problem is constituted by possible trade-offs between different kinds of responsibility. It is evident that attributing backward-looking responsibility for product failures diminishes the willingness to learn about such defects and to take forward-looking responsibility. We will argue that these problems can be overcome by elaborating what it is exactly that innovators are responsible for. In this manner, we will distinguish more clearly between holding responsible and taking responsibility. This opens a space for `supererogatory' responsibilities. Second, we will argue that both innovation processes and outcomes can be objects of innovators' responsibility. Third, we will analyze different kinds of responsibility (blameworthiness, accountability, liability, obligation and virtue) and show that the functions of their attribution are not necessarily contradictory. Based on this conceptual refinement, we will argue that accountability, responsibility-as-virtue and the willingness to take responsibility are crucial for responsible innovation.},
  issn     = {1573-0964},
  doi      = {10.1007/s11229-018-01951-7},
  url      = {https://doi.org/10.1007/s11229-018-01951-7}
}

@book{IEEE2019,
  author  = {{The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems}},
  title   = {Ethically Aligned Design: A Vision for Prioritizing Human Well-being with Autonomous and Intelligent Systems},
  edition = {1},
  editor  = {IEEE},
  year    = {2019},
  url     = {https://standards.ieee.org/content/ieee-standards/en/industry-connections/ec/autonomous-systems.html}
}

@article{Burke2011,
  title        = {Recommender Systems: An Overview},
  volume       = {32},
  url          = {https://ojs.aaai.org/index.php/aimagazine/article/view/2361},
  doi          = {10.1609/aimag.v32i3.2361},
  abstractnote = {Recommender systems are tools for interacting with large and complex information spaces. They provide a personalized view of such spaces, prioritizing items likely to be of interest to the user. The field, christened in 1995, has grown enormously in the variety of problems addressed and techniques employed, as well as in its practical applications. Recommender systems research has incorporated a wide variety of artificial intelligence techniques including machine learning, data mining, user modeling, case-based reasoning, and constraint satisfaction, among others. Personalized recommendations are an important part of many on-line e-commerce applications such as Amazon.com, Netflix, and Pandora. This wealth of practical application experience has provided inspiration to researchers to extend the reach of recommender systems into new and challenging areas. The purpose of the articles in this special issue is to take stock of the current landscape of recommender systems research and identify directions the field is now taking. This article provides an overview of the current state of the field and introduces the various articles in the special issue.&lt;br /&gt;&lt;br /&gt;},
  number       = {3},
  journal      = {AI Magazine},
  author       = {Burke, Robin and Felfernig, Alexander and Göker, Mehmet H.},
  year         = {2011},
  month        = {Jun.},
  pages        = {13-18}
}

@article{Burke2002,
  author   = {Burke, Robin},
  title    = {Hybrid Recommender Systems: Survey and Experiments},
  journal  = {User Modeling and User-Adapted Interaction},
  year     = {2002},
  month    = {Nov},
  day      = {01},
  volume   = {12},
  number   = {4},
  pages    = {331-370},
  abstract = {Recommender systems represent user preferences for the purpose of suggesting items to purchase or examine. They have become fundamental applications in electronic commerce and information access, providing suggestions that effectively prune large information spaces so that users are directed toward those items that best meet their needs and preferences. A variety of techniques have been proposed for performing recommendation, including content-based, collaborative, knowledge-based and other techniques. To improve performance, these methods have sometimes been combined in hybrid recommenders. This paper surveys the landscape of actual and possible hybrid recommenders, and introduces a novel hybrid, EntreeC, a system that combines knowledge-based recommendation and collaborative filtering to recommend restaurants. Further, we show that semantic ratings obtained from the knowledge-based part of the system enhance the effectiveness of collaborative filtering.},
  issn     = {1573-1391},
  doi      = {10.1023/A:1021240730564},
  url      = {https://doi.org/10.1023/A:1021240730564}
}

@online{Rocca2019,
  author  = {Rocca, Baptiste and Rocca, Joseph},
  title   = {Introduction to recommender systems},
  journal = {Towards Data Science},
  year    = {2019},
  url     = {https://towardsdatascience.com/introduction-to-recommender-systems-6c66cf15ada}
}

@inbook{Barocas2019,
  title     = {Fairness and Machine Learning: Limitations and Opportunities},
  author    = {Solon Barocas and Moritz Hardt and Arvind Narayanan},
  publisher = {fairmlbook.org},
  note      = {\url{http://www.fairmlbook.org}},
  year      = {2019},
  chapter   = {3}
}

@article{Milano2020,
  author   = {Milano, Silvia
              and Taddeo, Mariarosaria
              and Floridi, Luciano},
  title    = {Recommender systems and their ethical challenges},
  journal  = {AI {\&} SOCIETY},
  year     = {2020},
  month    = {Dec},
  day      = {01},
  volume   = {35},
  number   = {4},
  pages    = {957-967},
  abstract = {This article presents the first, systematic analysis of the ethical challenges posed by recommender systems through a literature review. The article identifies six areas of concern, and maps them onto a proposed taxonomy of different kinds of ethical impact. The analysis uncovers a gap in the literature: currently user-centred approaches do not consider the interests of a variety of other stakeholders---as opposed to just the receivers of a recommendation---in assessing the ethical impacts of a recommender system.},
  issn     = {1435-5655},
  doi      = {10.1007/s00146-020-00950-y},
  url      = {https://doi.org/10.1007/s00146-020-00950-y}
}

@article{Bozdag2013,
  author   = {Bozdag, Engin},
  title    = {Bias in algorithmic filtering and personalization},
  journal  = {Ethics and Information Technology},
  year     = {2013},
  month    = {Sep},
  day      = {01},
  volume   = {15},
  number   = {3},
  pages    = {209-227},
  abstract = {Online information intermediaries such as Facebook and Google are slowly replacing traditional media channels thereby partly becoming the gatekeepers of our society. To deal with the growing amount of information on the social web and the burden it brings on the average user, these gatekeepers recently started to introduce personalization features, algorithms that filter information per individual. In this paper we show that these online services that filter information are not merely algorithms. Humans not only affect the design of the algorithms, but they also can manually influence the filtering process even when the algorithm is operational. We further analyze filtering processes in detail, show how personalization connects to other filtering techniques, and show that both human and technical biases are present in today's emergent gatekeepers. We use the existing literature on gatekeeping and search engine bias and provide a model of algorithmic gatekeeping.},
  issn     = {1572-8439},
  doi      = {10.1007/s10676-013-9321-6},
  url      = {https://doi.org/10.1007/s10676-013-9321-6}
}

@article{Bozdag2015,
  author   = {Bozdag, Engin
              and van den Hoven, Jeroen},
  title    = {Breaking the filter bubble: democracy and design},
  journal  = {Ethics and Information Technology},
  year     = {2015},
  month    = {Dec},
  day      = {01},
  volume   = {17},
  number   = {4},
  pages    = {249-265},
  abstract = {It has been argued that the Internet and social media increase the number of available viewpoints, perspectives, ideas and opinions available, leading to a very diverse pool of information. However, critics have argued that algorithms used by search engines, social networking platforms and other large online intermediaries actually decrease information diversity by forming so-called ``filter bubbles''. This may form a serious threat to our democracies. In response to this threat others have developed algorithms and digital tools to combat filter bubbles. This paper first provides examples of different software designs that try to break filter bubbles. Secondly, we show how norms required by two democracy models dominate the tools that are developed to fight the filter bubbles, while norms of other models are completely missing in the tools. The paper in conclusion argues that democracy itself is a contested concept and points to a variety of norms. Designers of diversity enhancing tools must thus be exposed to diverse conceptions of democracy.},
  issn     = {1572-8439},
  doi      = {10.1007/s10676-015-9380-y},
  url      = {https://doi.org/10.1007/s10676-015-9380-y}
}

@misc{Stray2021,
  doi       = {10.48550/ARXIV.2107.04953},
  url       = {https://arxiv.org/abs/2107.04953},
  author    = {Stray, Jonathan},
  keywords  = {Information Retrieval (cs.IR), Computers and Society (cs.CY), Social and Information Networks (cs.SI), FOS: Computer and information sciences, FOS: Computer and information sciences, J.4; K.4.2},
  title     = {Designing Recommender Systems to Depolarize},
  publisher = {arXiv},
  year      = {2021},
  copyright = {Creative Commons Attribution Non Commercial No Derivatives 4.0 International}
}